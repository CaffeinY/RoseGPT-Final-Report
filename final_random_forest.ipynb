{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    '''\n",
    "        Convert unix time to informative time array\n",
    "        Input: unix time \n",
    "        Output: dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "    '''\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.weekday()\n",
    "\n",
    "def polyline_to_trip_duration(polyline):\n",
    "    '''\n",
    "        Convert polyline to time duration\n",
    "    '''\n",
    "    return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "def visualize_data(Xs, ys, title=\"\"):\n",
    "    plt.figure(figsize=(12,9))\n",
    "    plt.axhline(color=\"red\")\n",
    "    plt.axvline(color=\"red\")\n",
    "    for points_idx, (X, y) in enumerate(zip(Xs, ys)):\n",
    "        plt.scatter(X, y, s=10, c=colors[points_idx])\n",
    "    if title:\n",
    "        plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"X\", fontsize=18)\n",
    "    plt.ylabel(\"Y\", fontsize=18)\n",
    "    \n",
    "def expandTaxiStand(x):\n",
    "    stand_name, stand_lat, stand_lng = taxiStand_to_geo[x[\"ORIGIN_STAND\"]]\n",
    "    return stand_name, stand_lat, stand_lng\n",
    "\n",
    "def unstandardize_data(standardized_data, original_mean, original_std):\n",
    "    original_data = [(value * original_std) + original_mean for value in standardized_data]\n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Geo data\n",
    "# df_taxiStand = pd.read_csv(\"dataset/metaData_taxistandsID_name_GPSlocation.csv\")\n",
    "# # convert the meta information to dict\n",
    "# taxiStand_to_geo = {0:(\"None\", 0, 0)}\n",
    "# for _, row in df_taxiStand.iterrows():\n",
    "#     # taxiStand_to_geo[id] = (stand name, lat, lng)\n",
    "#     taxiStand_to_geo[row[0]] = (row[1], float(row[2]), float(row[3]))\n",
    "    \n",
    "\n",
    "# # Read data and select some columns\n",
    "# # We currently select not all columns\n",
    "# df_train = pd.read_csv(\"dataset/train.csv\")\n",
    "# df_train = df_train.fillna(0)\n",
    "# df_train[[\"YR\", \"MON\", \"DAY\", \"HR\", \"MIN\", \"SEC\",\"WK\"]] = df_train[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "# df_train[\"TIME_DURATION\"] = df_train[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "# df_train = pd.get_dummies(df_train, columns = ['CALL_TYPE'])\n",
    "# df_train = df_train.drop(['DAY_TYPE', 'TIMESTAMP'], axis=1)\n",
    "# df_train[[\"STAND_NAME\", \"STAND_LAT\", \"STAND_LNG\"]] = df_train[[\"ORIGIN_STAND\"]].apply(expandTaxiStand, axis=1, result_type=\"expand\")\n",
    "# df_train = df_train.drop_duplicates()\n",
    "\n",
    "# # shuffle and select out the validation set\n",
    "# df_train_sub = df_train.sample(frac=1.0)\n",
    "# train_valid_cutoff = 1000\n",
    "# df_valid = df_train_sub[0:train_valid_cutoff].copy()\n",
    "# df_train = df_train_sub[train_valid_cutoff:].copy()\n",
    "\n",
    "\n",
    "# # loading the testset\n",
    "# df_test = pd.read_csv(\"dataset/test_public.csv\")\n",
    "# df_test = df_test.fillna(0)\n",
    "# df_test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"MIN\", \"SEC\",\"WK\"]] = df_test[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "# df_test = pd.get_dummies(df_test, columns = ['CALL_TYPE'])\n",
    "# df_test = df_test.drop(['DAY_TYPE', 'TIMESTAMP'], axis=1)\n",
    "# df_test[[\"STAND_NAME\", \"STAND_LAT\", \"STAND_LNG\"]] = df_test[[\"ORIGIN_STAND\"]].apply(expandTaxiStand, axis=1, result_type=\"expand\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##\n",
    "# # global dictionaries for mapping id to index\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Taxi ID\n",
    "# taxiId = sorted(list(set(df_train[\"TAXI_ID\"].unique())))\n",
    "# taxiId_to_ix = defaultdict(lambda: 0, { id:i+1 for i,id in enumerate(taxiId)})\n",
    "# ix_to_taxiId = { i+1:id for i,id in enumerate(taxiId)}\n",
    "\n",
    "# df_train[\"TAXI_ID_ix\"] = df_train[\"TAXI_ID\"].apply(lambda x : taxiId_to_ix[x])\n",
    "# df_test[\"TAXI_ID_ix\"] = df_test[\"TAXI_ID\"].apply(lambda x : taxiId_to_ix[x])\n",
    "# df_valid[\"TAXI_ID_ix\"] = df_valid[\"TAXI_ID\"].apply(lambda x : taxiId_to_ix[x])\n",
    "\n",
    "# # Call ID\n",
    "# callId = sorted(list(set(df_train[\"ORIGIN_CALL\"].unique())))[1:] # remove 0 in the first\n",
    "# callId_to_ix = defaultdict(lambda: 0, { id:i+1 for i,id in enumerate(callId)})\n",
    "# ix_to_callId = { i+1:id for i,id in enumerate(callId)}\n",
    "\n",
    "# df_train[\"CALL_ID_ix\"] = df_train[\"ORIGIN_CALL\"].apply(lambda x : callId_to_ix[x])\n",
    "# df_test[\"CALL_ID_ix\"] = df_test[\"ORIGIN_CALL\"].apply(lambda x : callId_to_ix[x])\n",
    "# df_valid[\"CALL_ID_ix\"] = df_valid[\"ORIGIN_CALL\"].apply(lambda x : callId_to_ix[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced preprocessing to find out 60k representive data\n",
    "# 17: 40%, 8:10% 3:10%, 14:10% 24000\n",
    "# bool_17 = df_train[\"HR\"] == 17\n",
    "# bool_8 = df_train[\"HR\"] == 8\n",
    "# bool_3 = df_train[\"HR\"] == 3\n",
    "# bool_14 = df_train[\"HR\"] == 14\n",
    "\n",
    "# df_hr_17 = df_train[bool_17].sample(n = 18000)\n",
    "# df_hr_8 = df_train[bool_8].sample(n = 9000)\n",
    "# df_hr_3 = df_train[bool_3].sample(n = 8000)\n",
    "# df_hr_14 = df_train[bool_14].sample(n = 8000)\n",
    "# df_hr_others = df_train[~bool_17 &  ~bool_8 & ~bool_3 & ~bool_14].sample(n = 17000)\n",
    "                    \n",
    "# df_subTrain = pd.concat([df_hr_17, df_hr_8, df_hr_3,df_hr_14, df_hr_others])\n",
    "# df_train = df_subTrain.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save df_train and df_valid for thurther load\n",
    "# df_train.to_csv('final/dataframe/df_train_new.csv', index=False)\n",
    "# df_valid.to_csv('final/dataframe/df_valid.csv', index=False)\n",
    "# df_test.to_csv('final/dataframe/df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qucik load without processing again\n",
    "df_train = pd.read_csv(\"final/dataframe/df_train.csv\")\n",
    "df_valid = pd.read_csv(\"final/dataframe/df_valid.csv\")\n",
    "df_test = pd.read_csv(\"final/dataframe/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize time duration\n",
    "time_mean = df_train['TIME_DURATION'].mean()\n",
    "time_std = df_train['TIME_DURATION'].std()\n",
    "\n",
    "df_train['TIME_DURATION_STD'] = (df_train['TIME_DURATION'] - df_train['TIME_DURATION'].mean()) / df_train['TIME_DURATION'].std()\n",
    "\n",
    "# Standardize LAT/lng duration\n",
    "df_train['LAT_STD'] = (df_train['STAND_LAT'] - df_train['STAND_LAT'].mean()) / df_train['STAND_LAT'].std()\n",
    "df_train['LNG_STD'] = (df_train['STAND_LNG'] - df_train['STAND_LNG'].mean()) / df_train['STAND_LNG'].std()\n",
    "\n",
    "# Standardize test\n",
    "df_test['TIME_DURATION_STD'] = (df_train['TIME_DURATION'] - df_train['TIME_DURATION'].mean()) / df_train['TIME_DURATION'].std()\n",
    "df_test['LAT_STD'] = (df_test['STAND_LAT'] - df_train['STAND_LAT'].mean()) / df_train['STAND_LAT'].std()\n",
    "df_test['LNG_STD'] = (df_test['STAND_LNG'] - df_train['STAND_LNG'].mean()) / df_train['STAND_LNG'].std()\n",
    "\n",
    "# Standardize valid\n",
    "df_valid['TIME_DURATION_STD'] = (df_valid['TIME_DURATION'] - df_train['TIME_DURATION'].mean()) / df_train['TIME_DURATION'].std()\n",
    "df_valid['LAT_STD'] = (df_valid['STAND_LAT'] - df_train['STAND_LAT'].mean()) / df_train['STAND_LAT'].std()\n",
    "df_valid['LNG_STD'] = (df_valid['STAND_LNG'] - df_train['STAND_LNG'].mean()) / df_train['STAND_LNG'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['MON','HR', 'WK', 'ORIGIN_STAND',\"MIN\",'SEC', 'CALL_TYPE_A', 'CALL_TYPE_B', \n",
    "                   'CALL_TYPE_C']\n",
    "\n",
    "\n",
    "df_features = df_train[feature_columns].copy()\n",
    "df_features[\"ORIGIN_STAND\"] = df_features['ORIGIN_STAND'].astype(int)\n",
    "# concatenate the time-duration column\n",
    "df_features = pd.concat([df_features, df_train['TIME_DURATION_STD']], axis = 1)\n",
    "\n",
    "# df_features_test\n",
    "df_features_test = df_test[feature_columns].copy()\n",
    "df_features_valid = df_valid[feature_columns].copy()\n",
    "df_features_valid = pd.concat([df_features_valid, df_valid['TIME_DURATION_STD']], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Foresst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# due to the compute power, random forest cannot too much data sample\n",
    "\n",
    "# Create\n",
    "rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     max_depth=None, \n",
    "                                     min_samples_split=20,\n",
    "                                     max_features=7,\n",
    "                                     warm_start=False)\n",
    "\n",
    "\n",
    "rf_regressor.fit(df_features.iloc[:60000,:-1], df_features.iloc[:60000,-1])\n",
    "\n",
    "\n",
    "# Train ERROR\n",
    "y_pred = rf_regressor.predict(df_features.iloc[:60000,:-1])\n",
    "y_pred_unstandardize = np.array(unstandardize_data(y_pred, time_mean, time_std))\n",
    "y_actual_unstandardize = np.array(unstandardize_data(df_features.iloc[:60000,-1], time_mean, time_std))\n",
    "print(\"train RMSE:\", np.round(np.sqrt(np.mean((y_actual_unstandardize - y_pred_unstandardize)**2)), 3))\n",
    "\n",
    "# VALID ERROR\n",
    "y_pred = rf_regressor.predict(df_features_valid.iloc[:,:-1])\n",
    "y_pred_unstandardize = np.array(unstandardize_data(y_pred, time_mean, time_std))\n",
    "y_actual_unstandardize = np.array(unstandardize_data(df_features_valid.iloc[:,-1], time_mean, time_std))\n",
    "print(\"valid RMSE:\", np.round(np.sqrt(np.mean((y_actual_unstandardize - y_pred_unstandardize)**2)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # save this model\n",
    "# with open('final/rf_0.pkl', 'wb') as file:\n",
    "#     pickle.dump(rf_regressor, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final/rf_6.pkl', 'rb') as file:\n",
    "    rf_regressor = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE: 552.493\n",
      "valid RMSE: 886.28\n"
     ]
    }
   ],
   "source": [
    "# Train ERROR\n",
    "y_pred = rf_regressor.predict(df_features.iloc[:60000,:-1])\n",
    "y_pred_unstandardize = np.array(unstandardize_data(y_pred, time_mean, time_std))\n",
    "y_actual_unstandardize = np.array(unstandardize_data(df_features.iloc[:60000,-1], time_mean, time_std))\n",
    "print(\"train RMSE:\", np.round(np.sqrt(np.mean((y_actual_unstandardize - y_pred_unstandardize)**2)), 3))\n",
    "\n",
    "# VALID ERROR\n",
    "y_pred = rf_regressor.predict(df_features_valid.iloc[:,:-1])\n",
    "y_pred_unstandardize = np.array(unstandardize_data(y_pred, time_mean, time_std))\n",
    "y_actual_unstandardize = np.array(unstandardize_data(df_features_valid.iloc[:,-1], time_mean, time_std))\n",
    "print(\"valid RMSE:\", np.round(np.sqrt(np.mean((y_actual_unstandardize - y_pred_unstandardize)**2)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final/submit_rf0.csv'\n",
    "\n",
    "y_pred_test = rf_regressor.predict(df_features_test)\n",
    "predict_lst = np.array(unstandardize_data(y_pred_test, time_mean, time_std))\n",
    "result = []\n",
    "for i in predict_lst:\n",
    "    result.append(i)\n",
    "submit = pd.read_csv(\"dataset/sampleSubmission.csv\")\n",
    "submit['TRAVEL_TIME'] = result\n",
    "submit.to_csv(filename, sep=',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
